Given a tree, and the cost of a subtree is defined as |S|*AND(S) where |S| is the size of the subtree and AND(S) is bitwise AND of all indices of nodes from the subtree, task is to find maximum cost of possible subtree.
Prerequisite : Disjoint Set Union
Examples:
Approach : The strategy is to fix the AND, and find the maximum size of a subtree such that AND of all indices equals to the given AND. Suppose we fix AND as ‘A’. In binary representation of A, if the ith bit is ‘1’, then all indices(nodes) of the required subtree should have ‘1’ in ith position in binary representation. If ith bit is ‘0’ then indices either have ‘0’ or ‘1’ in ith position. That means all elements of subtree are super masks of A. All super masks of A can be generated in O(2^k) time where ‘k’ is the number of bits which are ‘0’ in A.
Now, the maximum size of subtree with a given AND ‘A’ can be found using DSU on the tree. Let, ‘u’ be the super mask of A and ‘p[u]’ be the parent of u. If p[u] is also a super mask of A, then, we have to update the DSU by merging the components of u and p[u]. Simultaneously, we also have to keep track of the maximum size of subtree. DSU helps us to do it. It will be more clear if we look at following code.
Run on IDE
Time Complexity : Union in DSU takes O(1) time. Generating all supermasks takes O(3^k) time where k is the maximum number of bits which are ‘0’. DFS takes O(n). Overall time complexity is O(3^k+n).
